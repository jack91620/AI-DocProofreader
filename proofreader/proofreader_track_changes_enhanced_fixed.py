#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¿®å¤ç‰ˆå¢å¼ºæ ¡å¯¹å™¨ - ç¡®ä¿è·Ÿè¸ªæ›´æ”¹å’Œæ‰¹æ³¨éƒ½æ­£ç¡®æ˜¾ç¤º
"""

import os
import sys
from typing import Optional
from rich.console import Console
from docx import Document
from datetime import datetime
import zipfile
import tempfile
import xml.etree.ElementTree as ET

from .config import Config
from .document import DocumentProcessor
from .ai_checker import AIChecker, ProofreadingResult
from .word_track_changes import WordTrackChangesManager, enable_track_changes_in_docx
from .word_comments_advanced import WordCommentsManager
from .word_comments_xml import create_comments_xml, create_document_rels, update_content_types


class ProofReaderWithTrackChangesAndCommentsFixed:
    """ä¿®å¤ç‰ˆå¢å¼ºæ ¡å¯¹å™¨ - ç¡®ä¿è·Ÿè¸ªæ›´æ”¹å’Œæ‰¹æ³¨éƒ½æ­£ç¡®æ˜¾ç¤º"""
    
    def __init__(self, api_key: str = None):
        """åˆå§‹åŒ–æ ¡å¯¹å™¨"""
        if api_key:
            # å¦‚æœä¼ å…¥äº†APIå¯†é’¥ï¼Œè®¾ç½®ç¯å¢ƒå˜é‡
            import os
            os.environ['OPENAI_API_KEY'] = api_key
        self.config = Config()
        self.ai_checker = AIChecker(self.config)
        self.console = Console()
        self.document_processor = DocumentProcessor()
    
    def proofread_with_track_changes_and_comments(self, input_file: str, output_file: str = None) -> bool:
        """ä½¿ç”¨è·Ÿè¸ªæ›´æ”¹å’Œæ‰¹æ³¨è¿›è¡Œæ ¡å¯¹ - ä¿®å¤ç‰ˆ"""
        try:
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            if not output_file:
                output_file = input_file.replace('.docx', '_enhanced_fixed.docx')
            
            self.console.print(f"[green]å¼€å§‹ä¿®å¤ç‰ˆå¢å¼ºæ ¡å¯¹ï¼š{input_file}[/green]")
            
            # ç¬¬ä¸€æ­¥ï¼šAIæ ¡å¯¹
            self.console.print("[blue]ç¬¬ä¸€æ­¥ï¼šAIæ ¡å¯¹åˆ†ææ–‡æ¡£...[/blue]")
            doc = Document(input_file)
            text_content = self.extract_text_content(doc)
            self.console.print(f"[blue]æå–æ–‡æœ¬å†…å®¹: {len(text_content)} ä¸ªæ®µè½[/blue]")
            
            ai_result = self.ai_checker.check_text(' '.join(text_content))
            
            # ç¬¬äºŒæ­¥ï¼šåˆ›å»ºåŒæ­¥æ›´æ”¹æ•°æ®
            self.console.print("[blue]ç¬¬äºŒæ­¥ï¼šç”ŸæˆåŒæ­¥æ›´æ”¹æ•°æ®...[/blue]")
            synchronized_changes = self._create_synchronized_changes(ai_result, text_content, doc)
            self.console.print(f"[green]âœ… å‘ç° {len(synchronized_changes)} ä¸ªéœ€è¦ä¿®æ”¹çš„é—®é¢˜[/green]")
            
            # ç¬¬ä¸‰æ­¥ï¼šåº”ç”¨æ›´æ”¹å’Œæ‰¹æ³¨
            self.console.print("[blue]ç¬¬ä¸‰æ­¥ï¼šåº”ç”¨è·Ÿè¸ªæ›´æ”¹å’Œæ‰¹æ³¨...[/blue]")
            success = self._apply_changes_with_proper_comments(doc, synchronized_changes, output_file)
            
            if success:
                self.console.print(f"[green]âœ… ä¿®å¤ç‰ˆå¢å¼ºæ ¡å¯¹å®Œæˆï¼š{output_file}[/green]")
                self.console.print("[blue]ğŸ“ æ–‡æ¡£åŒ…å«ï¼š[/blue]")
                self.console.print("   - ğŸ”„ Wordè·Ÿè¸ªæ›´æ”¹ï¼ˆå¯æ¥å—/æ‹’ç»ï¼‰")
                self.console.print("   - ğŸ’¬ å¯¹åº”çš„è¯¦ç»†æ‰¹æ³¨ï¼ˆå¯æŸ¥çœ‹/å›å¤ï¼‰")
                self.console.print("   - ğŸ”— æ­£ç¡®çš„æ‰¹æ³¨å¼•ç”¨é“¾æ¥")
                return True
            else:
                return False
            
        except Exception as e:
            self.console.print(f"[red]âŒ ä¿®å¤ç‰ˆå¢å¼ºæ ¡å¯¹å¤±è´¥: {e}[/red]")
            import traceback
            traceback.print_exc()
            return False

    def _create_synchronized_changes(self, ai_result: ProofreadingResult, text_content: list, doc: Document):
        """åˆ›å»ºåŒæ­¥çš„è·Ÿè¸ªæ›´æ”¹å’Œæ‰¹æ³¨æ•°æ®"""
        synchronized_changes = []
        processed_pairs = set()  # é¿å…é‡å¤å¤„ç†ç›¸åŒçš„ä¿®æ­£å¯¹
        
        self.console.print(f"[blue]ğŸ” å¤„ç†AIå‘ç°çš„ {len(ai_result.issues)} ä¸ªé—®é¢˜å’Œ {len(ai_result.suggestions)} ä¸ªå»ºè®®[/blue]")
        
        # åˆ›å»ºæ‰€æœ‰å¯èƒ½çš„ä¿®æ­£å¯¹
        all_corrections = []
        
        # ä»suggestionsä¸­æå–ä¿®æ­£å¯¹
        for suggestion in ai_result.suggestions:
            original_text = suggestion.get('original', '')
            suggested_text = suggestion.get('suggested', '')
            reason = suggestion.get('reason', '')
            
            if suggested_text and suggested_text != original_text:
                # æå–å…·ä½“çš„è¯æ±‡ä¿®æ­£
                corrections = self._extract_word_corrections(original_text, suggested_text)
                for orig, corr in corrections:
                    correction_pair = (orig, corr)
                    if correction_pair not in processed_pairs:
                        all_corrections.append({
                            'original': orig,
                            'corrected': corr,
                            'reason': reason,
                            'type': 'suggestion',
                            'full_original': original_text,
                            'full_suggested': suggested_text
                        })
                        processed_pairs.add(correction_pair)
        
        # ä»issuesä¸­æå–ä¿®æ­£å¯¹
        for issue in ai_result.issues:
            problem_text = issue.get('text', '')
            suggestion = issue.get('suggestion', '')
            issue_type = issue.get('type', '')
            severity = issue.get('severity', '')
            
            if issue_type == "æœ¯è¯­ä¸ä¸€è‡´" and "å‘ç°å¤šç§æœ¯è¯­ï¼š" in problem_text:
                # å¤„ç†æœ¯è¯­ä¸ä¸€è‡´
                terms = self._extract_terms_from_inconsistency(problem_text, suggestion)
                for original_term, corrected_term in terms:
                    correction_pair = (original_term, corrected_term)
                    if correction_pair not in processed_pairs:
                        all_corrections.append({
                            'original': original_term,
                            'corrected': corrected_term,
                            'reason': f"{issue_type} - {severity}",
                            'type': 'terminology',
                            'suggestion_text': suggestion
                        })
                        processed_pairs.add(correction_pair)
            elif issue_type in ["é”™åˆ«å­—å’Œç”¨è¯ä¸å½“", "æ ‡ç‚¹ç¬¦å·ä½¿ç”¨"]:
                # å¤„ç†é”™åˆ«å­—å’Œæ ‡ç‚¹é—®é¢˜
                corrected_text = self._extract_corrected_text(suggestion)
                if corrected_text and corrected_text != problem_text:
                    correction_pair = (problem_text, corrected_text)
                    if correction_pair not in processed_pairs:
                        all_corrections.append({
                            'original': problem_text,
                            'corrected': corrected_text,
                            'reason': f"{issue_type} - {severity}",
                            'type': 'error_fix',
                            'suggestion_text': suggestion
                        })
                        processed_pairs.add(correction_pair)
        
        # åˆ›å»ºæ®µè½ç´¢å¼•æ˜ å°„ - ä»éç©ºæ®µè½ç´¢å¼•åˆ°å®é™…æ®µè½ç´¢å¼•
        paragraph_mapping = {}
        text_index = 0
        for doc_index, paragraph in enumerate(doc.paragraphs):
            if paragraph.text.strip():
                paragraph_mapping[text_index] = doc_index
                text_index += 1
        
        # æ”¹è¿›çš„æ–‡æœ¬åŒ¹é…å’Œåº”ç”¨é€»è¾‘
        for correction in all_corrections:
            original = correction['original']
            corrected = correction['corrected']
            reason = correction['reason']
            corr_type = correction['type']
            
            # åœ¨æ‰€æœ‰æ®µè½ä¸­æŸ¥æ‰¾åŒ¹é…é¡¹
            matches_found = []
            for i, paragraph_text in enumerate(text_content):
                # ä½¿ç”¨æ›´ç²¾ç¡®çš„åŒ¹é…ç­–ç•¥
                if self._is_text_match(original, paragraph_text):
                    actual_paragraph_index = paragraph_mapping.get(i, i)
                    # è®¡ç®—è¯¥æœ¯è¯­åœ¨æ®µè½ä¸­å‡ºç°çš„æ¬¡æ•°
                    occurrences = paragraph_text.count(original)
                    matches_found.append((i, actual_paragraph_index, paragraph_text, occurrences))
            
            # å¤„ç†æ‰€æœ‰åŒ¹é…é¡¹
            if matches_found:
                for text_idx, para_idx, para_text, occurrences in matches_found:
                    # ä¸ºæ¯ä¸ªå‡ºç°çš„æœ¯è¯­åˆ›å»ºä¸€ä¸ªä¿®æ­£é¡¹
                    for occurrence in range(occurrences):
                        # åˆ›å»ºæ‰¹æ³¨æ–‡æœ¬
                        if corr_type == 'suggestion':
                            comment_text = f"ğŸ’¡ æ”¹è¿›å»ºè®®: {original} â†’ {corrected}\n"
                            comment_text += f"ğŸ“‹ åŸå› : {reason}\n"
                            comment_text += f"ğŸ¯ ç±»å‹: æ”¹è¿›å»ºè®®\n"
                        elif corr_type == 'terminology':
                            comment_text = f"ğŸ” æœ¯è¯­ä¸ä¸€è‡´ä¿®æ­£: {original} â†’ {corrected}\n"
                            comment_text += f"ğŸ“ ç†ç”±: {reason}\n"
                            comment_text += f"ğŸ’¡ å»ºè®®: {correction.get('suggestion_text', '')}\n"
                        else:
                            comment_text = f"ğŸ”§ é”™è¯¯ä¿®æ­£: {original} â†’ {corrected}\n"
                            comment_text += f"ğŸ“ ç†ç”±: {reason}\n"
                            comment_text += f"ğŸ’¡ å»ºè®®: {correction.get('suggestion_text', '')}\n"
                        
                        comment_text += f"â° å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}"
                        
                        synchronized_changes.append({
                            'paragraph_index': para_idx,
                            'original_text': original,
                            'corrected_text': corrected,
                            'comment_text': comment_text,
                            'reason': reason,
                            'type': corr_type,
                            'occurrence_index': occurrence  # æ·»åŠ å‡ºç°æ¬¡æ•°ç´¢å¼•
                        })
                        
                        self.console.print(f"[green]âœ… æ·»åŠ ä¿®æ­£: {original} â†’ {corrected} (æ®µè½{para_idx+1}, ç¬¬{occurrence+1}æ¬¡å‡ºç°)[/green]")
            else:
                self.console.print(f"[yellow]âš ï¸  æœªæ‰¾åˆ°åŒ¹é…æ–‡æœ¬: {original}[/yellow]")
        
        self.console.print(f"[green]âœ… æ€»å…±åˆ›å»ºäº† {len(synchronized_changes)} ä¸ªåŒæ­¥æ›´æ”¹[/green]")
        return synchronized_changes

    def _is_text_match(self, target_text: str, paragraph_text: str) -> bool:
        """æ”¹è¿›çš„æ–‡æœ¬åŒ¹é…é€»è¾‘"""
        # ç²¾ç¡®åŒ¹é…
        if target_text in paragraph_text:
            return True
        
        # å»é™¤æ ‡ç‚¹ç¬¦å·ååŒ¹é…
        import re
        target_clean = re.sub(r'[^\w\s]', '', target_text)
        paragraph_clean = re.sub(r'[^\w\s]', '', paragraph_text)
        if target_clean in paragraph_clean:
            return True
        
        # åˆ†è¯åŒ¹é…ï¼ˆå¤„ç†æœ¯è¯­ï¼‰
        target_words = target_text.split()
        if len(target_words) == 1 and target_words[0] in paragraph_text:
            return True
        
        return False

    def _apply_changes_with_proper_comments(self, doc: Document, synchronized_changes: list, output_file: str) -> bool:
        """åº”ç”¨æ›´æ”¹å¹¶ç¡®ä¿æ‰¹æ³¨æ­£ç¡®æ˜¾ç¤º"""
        try:
            # åˆ›å»ºè·Ÿè¸ªæ›´æ”¹ç®¡ç†å™¨
            track_changes_manager = WordTrackChangesManager(doc)
            
            # åˆ›å»ºæ‰¹æ³¨ç®¡ç†å™¨
            comments_manager = WordCommentsManager(doc)
            
            # åº”ç”¨æ¯ä¸ªæ›´æ”¹
            applied_count = 0
            for change in synchronized_changes:
                paragraph_index = change.get('paragraph_index', 0)
                original_text = change.get('original_text', '')
                corrected_text = change.get('corrected_text', '')
                comment_text = change.get('comment_text', '')
                reason = change.get('reason', '')
                
                if paragraph_index < len(doc.paragraphs):
                    paragraph = doc.paragraphs[paragraph_index]
                    
                    # åº”ç”¨è·Ÿè¸ªæ›´æ”¹
                    track_success = track_changes_manager.add_tracked_change(
                        paragraph, original_text, corrected_text, reason
                    )
                    
                    # åº”ç”¨æ‰¹æ³¨
                    comment_success = comments_manager.add_comment(
                        paragraph, original_text, comment_text
                    )
                    
                    if track_success and comment_success:
                        applied_count += 1
                        self.console.print(f"[green]âœ… åº”ç”¨æ›´æ”¹ {applied_count}: {original_text} -> {corrected_text}[/green]")
            
            # å®Œæˆå¤„ç†
            track_changes_manager.apply_all_changes()
            comments_manager.finalize_document()
            
            # ä¿å­˜å¸¦æœ‰åŸºæœ¬æ›´æ”¹çš„æ–‡æ¡£
            temp_file = output_file.replace('.docx', '_temp.docx')
            doc.save(temp_file)
            
            # åˆ›å»ºå®Œæ•´çš„æ‰¹æ³¨ç³»ç»Ÿ
            success = self._create_complete_comment_system(
                temp_file, 
                output_file, 
                self._prepare_comments_with_changes(comments_manager.get_comments_for_xml(), synchronized_changes)
            )
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_file):
                os.remove(temp_file)
            
            if success:
                self.console.print(f"[green]âœ… æˆåŠŸåº”ç”¨ {applied_count} ä¸ªæ›´æ”¹å’Œæ‰¹æ³¨[/green]")
                return True
            else:
                return False
                
        except Exception as e:
            self.console.print(f"[red]âŒ åº”ç”¨æ›´æ”¹å¤±è´¥: {e}[/red]")
            import traceback
            traceback.print_exc()
            return False

    def _create_complete_comment_system(self, temp_file: str, output_file: str, comments_data: list) -> bool:
        """åˆ›å»ºå®Œæ•´çš„æ‰¹æ³¨ç³»ç»Ÿ"""
        try:
            self.console.print(f"[cyan]ğŸ”§ åˆ›å»ºå®Œæ•´çš„æ‰¹æ³¨ç³»ç»Ÿï¼ŒåŒ…å« {len(comments_data)} ä¸ªæ‰¹æ³¨[/cyan]")
            
            with zipfile.ZipFile(temp_file, 'r') as input_zip:
                with zipfile.ZipFile(output_file, 'w', zipfile.ZIP_DEFLATED) as output_zip:
                    # å¤åˆ¶åŸæœ‰æ–‡ä»¶ï¼Œä½†éœ€è¦ä¿®æ”¹document.xml
                    for item in input_zip.infolist():
                        if item.filename == 'word/document.xml':
                            # ä¿®æ”¹document.xmlæ·»åŠ æ‰¹æ³¨å¼•ç”¨
                            document_xml = input_zip.read(item.filename).decode('utf-8')
                            modified_document_xml = self._add_comment_references_to_document(document_xml, comments_data)
                            output_zip.writestr(item.filename, modified_document_xml.encode('utf-8'))
                        elif item.filename not in ['word/comments.xml', 'word/_rels/document.xml.rels', '[Content_Types].xml']:
                            output_zip.writestr(item, input_zip.read(item.filename))
                    
                    # åˆ›å»ºæ‰¹æ³¨XML
                    comments_xml = self._create_comments_xml(comments_data)
                    output_zip.writestr('word/comments.xml', comments_xml)
                    
                    # æ›´æ–°å…³ç³»æ–‡ä»¶
                    rels_xml = self._create_updated_rels(input_zip)
                    output_zip.writestr('word/_rels/document.xml.rels', rels_xml)
                    
                    # æ›´æ–°å†…å®¹ç±»å‹
                    content_types_xml = self._create_updated_content_types(input_zip)
                    output_zip.writestr('[Content_Types].xml', content_types_xml)
            
            self.console.print("[green]âœ… å®Œæ•´çš„æ‰¹æ³¨ç³»ç»Ÿåˆ›å»ºæˆåŠŸ[/green]")
            return True
            
        except Exception as e:
            self.console.print(f"[red]âŒ åˆ›å»ºå®Œæ•´æ‰¹æ³¨ç³»ç»Ÿå¤±è´¥: {e}[/red]")
            import traceback
            traceback.print_exc()
            return False

    def _create_comments_xml(self, comments_data: list) -> str:
        """åˆ›å»ºæ‰¹æ³¨XMLå†…å®¹"""
        xml_content = '''<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<w:comments xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main">'''
        
        for comment in comments_data:
            comment_id = comment.get('id', 1)
            author = comment.get('author', 'AIæ ¡å¯¹åŠ©æ‰‹')
            date = comment.get('date', datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ"))
            text = comment.get('text', '')
            
            xml_content += f'''
    <w:comment w:id="{comment_id}" w:author="{author}" w:date="{date}" w:initials="AI">
        <w:p>
            <w:r>
                <w:t>{text}</w:t>
            </w:r>
        </w:p>
    </w:comment>'''
        
        xml_content += '\n</w:comments>'
        return xml_content

    def _create_updated_rels(self, input_zip) -> str:
        """åˆ›å»ºæ›´æ–°çš„å…³ç³»æ–‡ä»¶"""
        try:
            # è¯»å–åŸå§‹å…³ç³»æ–‡ä»¶
            if 'word/_rels/document.xml.rels' in input_zip.namelist():
                rels_xml = input_zip.read('word/_rels/document.xml.rels').decode('utf-8')
            else:
                # åˆ›å»ºåŸºæœ¬çš„å…³ç³»æ–‡ä»¶
                rels_xml = '''<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<Relationships xmlns="http://schemas.openxmlformats.org/package/2006/relationships">
</Relationships>'''
            
            # æ£€æŸ¥æ˜¯å¦å·²åŒ…å«æ‰¹æ³¨å…³ç³»
            if 'comments.xml' not in rels_xml:
                # æ·»åŠ æ‰¹æ³¨å…³ç³»
                rels_xml = rels_xml.replace(
                    '</Relationships>',
                    '''    <Relationship Id="rIdComments" Type="http://schemas.openxmlformats.org/officeDocument/2006/relationships/comments" Target="comments.xml"/>
</Relationships>'''
                )
            
            return rels_xml
            
        except Exception as e:
            print(f"åˆ›å»ºå…³ç³»æ–‡ä»¶å¤±è´¥: {e}")
            # è¿”å›åŸºæœ¬çš„å…³ç³»æ–‡ä»¶
            return '''<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<Relationships xmlns="http://schemas.openxmlformats.org/package/2006/relationships">
    <Relationship Id="rIdComments" Type="http://schemas.openxmlformats.org/officeDocument/2006/relationships/comments" Target="comments.xml"/>
</Relationships>'''

    def _add_comment_references_to_document(self, document_xml: str, comments_data: list) -> str:
        """åœ¨document.xmlä¸­æ·»åŠ æ‰¹æ³¨å¼•ç”¨æ ‡è®°"""
        try:
            modified_xml = document_xml
            
            # ä¸ºæ¯ä¸ªæ‰¹æ³¨æ·»åŠ å¼•ç”¨æ ‡è®°
            for comment in comments_data:
                comment_id = comment['id']
                # ä»synchronized_changesä¸­è·å–åŸå§‹æ–‡æœ¬ä¿¡æ¯
                # è¿™é‡Œéœ€è¦æ”¹è¿›è·å–åŸå§‹æ–‡æœ¬çš„æ–¹æ³•
                
                # ç®€å•çš„æ–¹æ³•ï¼šåœ¨æ¯ä¸ªåŒ…å«è·Ÿè¸ªæ›´æ”¹çš„æ®µè½åæ·»åŠ æ‰¹æ³¨å¼•ç”¨
                import re
                
                # æŸ¥æ‰¾åŒ…å«åˆ é™¤æ ‡è®°çš„ä½ç½®ï¼Œåœ¨å…¶åæ·»åŠ æ‰¹æ³¨å¼•ç”¨
                del_pattern = f'(<w:del[^>]*w:id="{comment_id}"[^>]*>.*?</w:del>)'
                matches = re.finditer(del_pattern, modified_xml, re.DOTALL)
                
                for match in matches:
                    # åœ¨åˆ é™¤æ ‡è®°åæ·»åŠ æ‰¹æ³¨æ ‡è®°
                    comment_range_start = f'<w:commentRangeStart w:id="{comment_id}"/>'
                    comment_range_end = f'<w:commentRangeEnd w:id="{comment_id}"/>'
                    comment_reference = f'<w:r><w:commentReference w:id="{comment_id}"/></w:r>'
                    
                    replacement = f'{comment_range_start}{match.group(0)}{comment_range_end}{comment_reference}'
                    modified_xml = modified_xml.replace(match.group(0), replacement, 1)
                    self.console.print(f"[green]âœ… æ·»åŠ æ‰¹æ³¨å¼•ç”¨æ ‡è®°: comment_id={comment_id}[/green]")
                    break  # åªå¤„ç†ç¬¬ä¸€ä¸ªåŒ¹é…
            
            return modified_xml
            
        except Exception as e:
            self.console.print(f"[red]æ·»åŠ æ‰¹æ³¨å¼•ç”¨å¤±è´¥: {e}[/red]")
            import traceback
            traceback.print_exc()
            return document_xml

    def _prepare_comments_with_changes(self, comments_data: list, synchronized_changes: list) -> list:
        """å‡†å¤‡åŒ…å«æ›´æ”¹ä¿¡æ¯çš„æ‰¹æ³¨æ•°æ®"""
        try:
            enhanced_comments = []
            
            for i, comment in enumerate(comments_data):
                enhanced_comment = comment.copy()
                
                # ä»synchronized_changesä¸­æ‰¾åˆ°å¯¹åº”çš„æ›´æ”¹ä¿¡æ¯
                if i < len(synchronized_changes):
                    change = synchronized_changes[i]
                    enhanced_comment['original_text'] = change.get('original_text', '')
                    enhanced_comment['corrected_text'] = change.get('corrected_text', '')
                
                enhanced_comments.append(enhanced_comment)
            
            return enhanced_comments
            
        except Exception as e:
            self.console.print(f"[red]å‡†å¤‡æ‰¹æ³¨æ•°æ®å¤±è´¥: {e}[/red]")
            return comments_data

    def _create_updated_content_types(self, input_zip) -> str:
        """åˆ›å»ºæ›´æ–°çš„å†…å®¹ç±»å‹æ–‡ä»¶"""
        try:
            # è¯»å–åŸå§‹å†…å®¹ç±»å‹æ–‡ä»¶
            if '[Content_Types].xml' in input_zip.namelist():
                content_types_xml = input_zip.read('[Content_Types].xml').decode('utf-8')
            else:
                content_types_xml = '''<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<Types xmlns="http://schemas.openxmlformats.org/package/2006/content-types">
</Types>'''
            
            # æ£€æŸ¥æ˜¯å¦å·²åŒ…å«æ‰¹æ³¨å†…å®¹ç±»å‹
            if 'word/comments.xml' not in content_types_xml:
                # æ·»åŠ æ‰¹æ³¨å†…å®¹ç±»å‹
                content_types_xml = content_types_xml.replace(
                    '</Types>',
                    '''    <Override PartName="/word/comments.xml" ContentType="application/vnd.openxmlformats-officedocument.wordprocessingml.comments+xml"/>
</Types>'''
                )
            
            return content_types_xml
            
        except Exception as e:
            print(f"åˆ›å»ºå†…å®¹ç±»å‹æ–‡ä»¶å¤±è´¥: {e}")
            # è¿”å›åŸºæœ¬çš„å†…å®¹ç±»å‹æ–‡ä»¶
            return '''<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<Types xmlns="http://schemas.openxmlformats.org/package/2006/content-types">
    <Override PartName="/word/comments.xml" ContentType="application/vnd.openxmlformats-officedocument.wordprocessingml.comments+xml"/>
</Types>'''

    def _extract_word_corrections(self, original_text: str, suggested_text: str):
        """ä»å¥å­çº§åˆ«çš„ä¿®æ­£ä¸­æå–è¯æ±‡çº§åˆ«çš„ä¿®æ­£"""
        corrections = []
        try:
            # ç®€å•çš„è¯æ±‡å·®å¼‚æ£€æµ‹
            original_words = original_text.split()
            suggested_words = suggested_text.split()
            
            # å¦‚æœé•¿åº¦ç›¸åŒï¼Œé€è¯æ¯”è¾ƒ
            if len(original_words) == len(suggested_words):
                for orig, sugg in zip(original_words, suggested_words):
                    if orig != sugg:
                        corrections.append((orig, sugg))
            else:
                # å¦‚æœé•¿åº¦ä¸åŒï¼ŒæŸ¥æ‰¾æ˜æ˜¾çš„æ›¿æ¢
                common_replacements = [
                    ("è®¡ç®—å™¨ç§‘å­¦", "è®¡ç®—æœºç§‘å­¦"),
                    ("ç¨‹å¼è®¾è®¡", "ç¨‹åºè®¾è®¡"), 
                    ("è½¯ä½“å·¥ç¨‹", "è½¯ä»¶å·¥ç¨‹"),
                    ("å˜æ•°", "å˜é‡"),
                    ("å‡½å¼", "å‡½æ•°"),
                    ("è¶…çº§è®¡ç®—å™¨", "è¶…çº§è®¡ç®—æœº"),
                    (",", "ï¼Œ"),  # æ ‡ç‚¹ç¬¦å·æ›¿æ¢
                ]
                
                for orig, repl in common_replacements:
                    if orig in original_text and repl in suggested_text:
                        corrections.append((orig, repl))
            
            return corrections
        except Exception as e:
            self.console.print(f"[yellow]âš ï¸  æå–è¯æ±‡ä¿®æ­£å¤±è´¥: {e}[/yellow]")
            return []

    def _extract_terms_from_inconsistency(self, problem_text: str, suggestion: str):
        """ä»æœ¯è¯­ä¸ä¸€è‡´é—®é¢˜ä¸­æå–æœ¯è¯­å¯¹"""
        terms = []
        
        # è§£æä¸ä¸€è‡´æœ¯è¯­æè¿°
        if "å‘ç°å¤šç§æœ¯è¯­ï¼š" in problem_text:
            # æå–æœ¯è¯­åˆ—è¡¨
            terms_part = problem_text.split("å‘ç°å¤šç§æœ¯è¯­ï¼š")[1].strip()
            # ç§»é™¤å¯èƒ½çš„é¢å¤–æè¿°
            if "ï¼Œ" in terms_part:
                terms_part = terms_part.split("ï¼Œ")[0]
            if "ã€‚" in terms_part:
                terms_part = terms_part.split("ã€‚")[0]
            
            # åˆ†å‰²æœ¯è¯­
            term_variants = []
            if "ã€" in terms_part:
                term_variants = [t.strip().strip('"').strip("'") for t in terms_part.split("ã€")]
            elif "ï¼Œ" in terms_part:
                term_variants = [t.strip().strip('"').strip("'") for t in terms_part.split("ï¼Œ")]
            else:
                # å•ä¸ªæœ¯è¯­çš„æƒ…å†µ
                term_variants = [terms_part.strip().strip('"').strip("'")]
            
            # ä»å»ºè®®ä¸­æå–æ ‡å‡†æœ¯è¯­
            standard_term = None
            if "å»ºè®®ç»Ÿä¸€ä½¿ç”¨" in suggestion:
                standard_part = suggestion.split("å»ºè®®ç»Ÿä¸€ä½¿ç”¨")[1].strip()
                if "ã€‚" in standard_part:
                    standard_part = standard_part.split("ã€‚")[0]
                if "ï¼Œ" in standard_part:
                    standard_part = standard_part.split("ï¼Œ")[0]
                standard_term = standard_part.strip().strip('"').strip("'")
            elif "æ¨èä½¿ç”¨" in suggestion:
                standard_part = suggestion.split("æ¨èä½¿ç”¨")[1].strip()
                if "ã€‚" in standard_part:
                    standard_part = standard_part.split("ã€‚")[0]
                if "ï¼Œ" in standard_part:
                    standard_part = standard_part.split("ï¼Œ")[0]
                standard_term = standard_part.strip().strip('"').strip("'")
            
            # å¦‚æœæ‰¾åˆ°æ ‡å‡†æœ¯è¯­ï¼Œä¸ºæ¯ä¸ªå˜ä½“åˆ›å»ºä¿®æ­£å¯¹
            if standard_term and term_variants:
                for variant in term_variants:
                    if variant and variant != standard_term:
                        terms.append((variant, standard_term))
                        self.console.print(f"[cyan]ğŸ“ æœ¯è¯­ä¿®æ­£: {variant} â†’ {standard_term}[/cyan]")
            
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„æ ‡å‡†æœ¯è¯­ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä½œä¸ºæ ‡å‡†
            elif len(term_variants) > 1:
                standard_term = term_variants[0]
                for variant in term_variants[1:]:
                    if variant and variant != standard_term:
                        terms.append((variant, standard_term))
                        self.console.print(f"[cyan]ğŸ“ æœ¯è¯­ä¿®æ­£: {variant} â†’ {standard_term}[/cyan]")
        
        # å¤„ç†ç‰¹æ®Šçš„æœ¯è¯­å¯¹
        special_corrections = {
            "è½¯ä½“å·¥ç¨‹": "è½¯ä»¶å·¥ç¨‹",
            "ç¨‹å¼è®¾è®¡": "ç¨‹åºè®¾è®¡", 
            "è®¡ç®—å™¨ç§‘å­¦": "è®¡ç®—æœºç§‘å­¦",
            "èµ„æ–™ç»“æ„": "æ•°æ®ç»“æ„",
            "æ¼”ç®—æ³•": "ç®—æ³•"
        }
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹æ®Šæœ¯è¯­
        for original, corrected in special_corrections.items():
            if original in problem_text or original in suggestion:
                terms.append((original, corrected))
                self.console.print(f"[cyan]ğŸ”§ ç‰¹æ®Šæœ¯è¯­ä¿®æ­£: {original} â†’ {corrected}[/cyan]")
        
        return terms

    def _extract_corrected_text(self, suggestion: str):
        """ä»å»ºè®®ä¸­æå–ä¿®æ­£åçš„æ–‡æœ¬"""
        if not suggestion:
            return None
        
        # å¸¸è§çš„ä¿®æ­£æ¨¡å¼
        patterns = [
            r"åº”ä¸º[ï¼š:]?\s*[\"']([^\"']+)[\"']",
            r"æ”¹ä¸º[ï¼š:]?\s*[\"']([^\"']+)[\"']", 
            r"ä¿®æ­£ä¸º[ï¼š:]?\s*[\"']([^\"']+)[\"']",
            r"å»ºè®®æ”¹ä¸º[ï¼š:]?\s*[\"']([^\"']+)[\"']",
            r"åº”è¯¥æ˜¯[ï¼š:]?\s*[\"']([^\"']+)[\"']",
            r"æ­£ç¡®çš„æ˜¯[ï¼š:]?\s*[\"']([^\"']+)[\"']",
            r"â†’\s*[\"']([^\"']+)[\"']",
            r"æ›¿æ¢ä¸º[ï¼š:]?\s*[\"']([^\"']+)[\"']"
        ]
        
        import re
        for pattern in patterns:
            match = re.search(pattern, suggestion)
            if match:
                corrected = match.group(1).strip()
                self.console.print(f"[cyan]ğŸ” æå–ä¿®æ­£æ–‡æœ¬: {corrected}[/cyan]")
                return corrected
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¼•å·åŒ…å›´çš„æ–‡æœ¬ï¼Œå°è¯•å…¶ä»–æ¨¡å¼
        simple_patterns = [
            r"åº”ä¸º[ï¼š:]?\s*([^\sï¼Œã€‚]+)",
            r"æ”¹ä¸º[ï¼š:]?\s*([^\sï¼Œã€‚]+)",
            r"ä¿®æ­£ä¸º[ï¼š:]?\s*([^\sï¼Œã€‚]+)",
            r"å»ºè®®æ”¹ä¸º[ï¼š:]?\s*([^\sï¼Œã€‚]+)",
            r"åº”è¯¥æ˜¯[ï¼š:]?\s*([^\sï¼Œã€‚]+)",
            r"æ­£ç¡®çš„æ˜¯[ï¼š:]?\s*([^\sï¼Œã€‚]+)"
        ]
        
        for pattern in simple_patterns:
            match = re.search(pattern, suggestion)
            if match:
                corrected = match.group(1).strip()
                self.console.print(f"[cyan]ğŸ” æå–ä¿®æ­£æ–‡æœ¬: {corrected}[/cyan]")
                return corrected
        
        # å¤„ç†ç‰¹æ®Šæƒ…å†µï¼šç›´æ¥çš„æ›¿æ¢å»ºè®®
        if "â†’" in suggestion:
            parts = suggestion.split("â†’")
            if len(parts) >= 2:
                corrected = parts[-1].strip().strip('"').strip("'").strip("ã€‚").strip("ï¼Œ")
                if corrected:
                    self.console.print(f"[cyan]ğŸ” æå–ä¿®æ­£æ–‡æœ¬: {corrected}[/cyan]")
                    return corrected
        
        self.console.print(f"[yellow]âš ï¸  æ— æ³•æå–ä¿®æ­£æ–‡æœ¬: {suggestion}[/yellow]")
        return None
    
    def extract_text_content(self, doc: Document):
        """æå–æ–‡æ¡£çš„æ–‡æœ¬å†…å®¹"""
        text_content = []
        for paragraph in doc.paragraphs:
            text_content.append(paragraph.text)
        return text_content


def test_fixed_enhanced_proofreader():
    """æµ‹è¯•ä¿®å¤ç‰ˆå¢å¼ºæ ¡å¯¹å™¨"""
    try:
        # ä½¿ç”¨æµ‹è¯•APIå¯†é’¥
        api_key = "sk-test"
        
        proofreader = ProofReaderWithTrackChangesAndCommentsFixed(api_key)
        
        input_file = "sample_input.docx"
        output_file = "output_fixed.docx"
        
        if os.path.exists(input_file):
            success = proofreader.proofread_with_track_changes_and_comments(input_file, output_file)
            if success:
                print(f"âœ… ä¿®å¤ç‰ˆå¢å¼ºæ ¡å¯¹æˆåŠŸ: {output_file}")
            else:
                print("âŒ ä¿®å¤ç‰ˆå¢å¼ºæ ¡å¯¹å¤±è´¥")
        else:
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    test_fixed_enhanced_proofreader() 